---
title: "Deliverable 3"
author: "Guillem Valls, Sergio Mazzariol"
output:
  word_document:
    toc: yes
    toc_depth: '3'
  pdf_document:
    number_sections: yes
    toc: yes
    toc_depth: '3'
  html_document:
    number_sections: yes
    toc: yes
    toc_depth: 3
editor_options:
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r, include=FALSE}
# Load Required Packages: to be increased over the course

requiredPackages <- c("mvoutlier","chemometrics","mice","missForest","missMDA","DMwR","pbkrtest","jomo","readxl","haven","sf","rgdal","missMDA","effects","FactoMineR","car","factoextra","RColorBrewer","ggplot2","dplyr","data.table", "ggmap","ggthemes","knitr","MVA","MASS")
missingPackages <- requiredPackages[!(requiredPackages %in% as.vector(installed.packages(lib.loc="~/R/win-library/3.5")[,"Package"]))]

if(length(missingPackages)) install.packages(missingPackages)
lapply(requiredPackages, require, character.only = TRUE)

```


```{r include=FALSE}
setwd("C:/Users/Sergio/Dropbox/UPC/FIB/Analisis de datos y explotacion de la informacion (ADEI)/FIB-ADEI-Big-Data-Analysis")
#setwd("C:/Users/usuario/Documents/ADEI/FIB-ADEI-Big-Data-Analysis")

rm(list=ls())
load("data-INI2.RData")
```


# Y (Numeric Target). This variable will be the target for linear model building (connected to blocks Statistical Modeling I and II).

## Explicative Variables for modeling purposes are generally (not all in this dataset):

    * Socioeconomic variables: gender, age, education, type of work, etc
    * Activity in the current commercial campaign
    * Bank marketing history
    * Economic vars

## Multivariant Analysis conducted in previous deliverables has to be used to select the initial model. Students have some degrees in freedom in model building, but the following conditions are requested:

    * At least two numerical variables have to be considered as explicative variables for initial steps in model building, called covariates. Non-linear models have to be checked for consistency.
    * Select the most significant factors found in Multivariant Data Analysis as initial model factors.  Put some reasonable limits to initial model complexity.
    * You have to consider at least one interaction between a couple of factors and one interaction between factor and covariate.
    * Diagnostics of the final model have to be undertaken. Lack of fit observations and influence data have to be selected and discussed (connections to multidimensional outliers in Multivariant Data Analysis is highly valuable)
    
# Clasificación de las variables (modeling I y II).
    *   Variable Target "duration"
    *   Socio económicas: age, job, marital, education, default, housing, loan
    *   Activity in the current commercial campaign: Month, day of the week, duration, campaing, pdays(? preguntar), previous(? preguntar).
    *   Bank marketing history: pdays, previous, poutcome.
    *   Economic vars: emp.var.rate, cons.price.index, cons.conf.index, euribor3m, number employed.

Si tuvieramos muchas variables explicativas podriamos utilizar el resultado del condes para saber cuales de ellas utilizar, aunque también sería posible seleccionarlas a partir del análisis de componentes principales. Dado que tenemos poca cantidad de variables usamos todas. Empezamos utilizando lm para crear un modelo inicial del cual podemos ir descartando aquellas variables explicativas que nos parecen irrelevantes, seguidamente comprobamos nuestra selección usando el step.

```{r}
m1<-lm(duration~.,data=df[,c("duration",vars_num)])
summary(m1)
Anova(m1)
```

Viendo este volcado, vemos que todas las variables menos, campaign tienen un p-value superior al 0.05, sin embargo, pdays y previous estan por debajo de 0.1 lo que podriamos llegar a incorporarlas al modelo. El r-square es de 0.006393 preguntar (que es r-square)

Al ver el resultado de Anova, podemos ver resultados muy parecidos preguntar (diferencia de Anova y summary)
    
```{r}
m2<-lm(duration~campaign+pdays+previous,data=df)
summary(m2)
Anova(m2)

m3<-lm(duration~campaign+pdays,data=df)
summary(m3)
Anova(m3)

vif(m3)

par(mfrow=c(2,2))
plot(m3)
par(mfrow=c(1,1))

m=m3;
```

Viendo el resultado del lm con estas variables, podemos ver que previous da por encima de 0.2, por lo que descartamos esta variable.

Al realizar nuevamente el lm con estas dos variables restantes, vemos que su p-value es inferior al 0.1, por lo que decidimos quedarnos con estas dos variables.

Ahora procedemos a comprobar el resultado usando el step, conocido como Akaike.

Hacemos el análisis de residuos con vif, el cual nos dice que si nos da valores por debajo de 3 son buenos y si nos da por encima de 5 quiere decir que las variables elejidas tienen redundancia y que inflará las varianzas. En nuestro caso, el resultado de las dos variables es inferior a 3.

Viendo el plota de la normal Q-Q, vemos que los valores distan mucho de la recta de referencia, con que podemos decir que su distribución no es para nada normal.

```{r}
m4<-step(m1) # preguntar porque no quita mas? y porque da diferente al nuestro
vif(m4)
par(mfrow=c(2,2))
plot(m4)
par(mfrow=c(1,1))
```

Podemos ver que el Akaike ha conservado las dos variables que nosotros hemos decidido dejar.

Hacemos el análisis de residuos con vif. En nuestro caso, el resultado de las 5 variables es inferior a 3.

Al igual que en nuestro caso nos da una plot Q-Q totalmente desviada de las dist normal.

```{r}
m5<-step(m1,k=log(nrow(df)))

par(mfrow=c(2,2))
plot(m5)
par(mfrow=c(1,1))
```

Si usamos la versión bayesiana (del BIC), la cual es conveniente usar en casos con muestras grandes, se queda solo con una sola variable (campaign) preguntar ya que el enunciado dice que nos quedemos por lo menos con dos.

En este caso no podemos hacer el análisis de residuos con vif porque solo tenemos 1 variable.

Al igual que en nuestro caso nos da una plot Q-Q totalmente desviada de las dist normal.


```{r}
boxcox(m,data=df)  #lambda 0 <- log(Y)

m6<-lm(log(duration)~.,data=df[,c("duration",vars_num)])
Anova(m6)

m7<-lm(log(duration)~campaign+pdays+nr.employed+euribor3m,data=df) # preguntar porque el vif da mas de 5, hay que quitar euribor o employed, si quitamos una la otra sube su pvalor.

summary(m7)
Anova(m7)

vif(m7)

par(mfrow=c(2,2))
plot(m7)
par(mfrow=c(1,1))
```

Aplicando el logaritmo, lo que estamos haciendo es la transformación.

Vemos que para emp.var.rate, nos resulta util usar su versión polinomica al cuadrado, ya que el p-value es significativamente menor pregunta esta bien este analisis de polinomios?

```{r}
m20<-lm(log(duration)~poly(campaign,2)+poly(pdays,2),data=df)
summary(m20)
Anova(m20)
```



# Outcome/Target : A binary response variable (Binary Target) will be the response variable for Binary Regression Models included in Statistical Modeling Part III.

Explicative Variables for modeling purposes are those available in dataset, exceptions will be indicated, if any.

Multivariant Analysis conducted in previous deliverables has to be used to select the initial model. Students have some degrees in freedom in model building, but the following conditions are requested:

    * Split the sample in work and test samples (consisting on a 80-20 split). Working data frame has to be used for model building purposes.

    *At least two numerical variables have to be considered as explicative variables for initial steps in model building.
    *Select the most significant factors according to feature selection as initial model factors.  Put some reasonable limits to initial model complexity.
    *You have to consider at least one interaction between a couple of factors and one interaction between factor and covariate.
    *Diagnostics of the final model have to be undertaken. Lack of fit observations and influence data have to be selected and discussed (connections to multidimensional outliers in Multivariant Data Analysis is highly valuable).
    *You have to predict Y (Binary Target)  in the Working Data Frame  vs the rest according to the best validated model that you can find and make a confusion matrix.
    *Make a confusion matrix in the Testing Data Frame for  Y  (Binary Target) according to the best validated model found.

```{r}

```



Confusion Matrix: When referring to the performance of a classification model, we are interested in the modelâ€™s ability to correctly predict or separate the classes. When looking at the errors made by a classification model, the confusion matrix gives the full picture. Consider e.g. a three class problem with the classes A, and B. The confusion matrix shows how the predictions are made by the model. The rows correspond to the known class of the data, i.e. the labels in the data. The columns correspond to the predictions made by the model. The value of each of element in the matrix is the number of predictions made with the class corresponding to the column for examples with the correct value as represented by the row. Thus, the diagonal elements show the number of correct classifications made for each class, and the off-diagonal elements show the errors made.