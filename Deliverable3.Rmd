---
title: "Deliverable 3"
author: "Guillem Valls, Sergio Mazzariol"
output:
  word_document:
    toc: yes
    toc_depth: '3'
  pdf_document:
    number_sections: yes
    toc: yes
    toc_depth: '3'
  html_document:
    number_sections: yes
    toc: yes
    toc_depth: 3
editor_options:
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r, include=FALSE}
# Load Required Packages: to be increased over the course

requiredPackages <- c("mvoutlier","chemometrics","mice","missForest","missMDA","DMwR","pbkrtest","jomo","readxl","haven","sf","rgdal","missMDA","effects","FactoMineR","car","factoextra","RColorBrewer","ggplot2","dplyr","data.table", "ggmap","ggthemes","knitr","MVA","MASS")
missingPackages <- requiredPackages[!(requiredPackages %in% as.vector(installed.packages(lib.loc="~/R/win-library/3.5")[,"Package"]))]

if(length(missingPackages)) install.packages(missingPackages)
lapply(requiredPackages, require, character.only = TRUE)

```


```{r include=FALSE}
if(Sys.info()[[4]]=="PORT_ELISABET"){
  setwd("C:/Users/usuario/Documents/ADEI/FIB-ADEI-Big-Data-Analysis")
}else{
  setwd("C:/Users/Sergio/Dropbox/UPC/FIB/Analisis de datos y explotacion de la informacion (ADEI)/FIB-ADEI-Big-Data-Analysis")
}

rm(list=ls())
load("data-INI2.RData")
```


El primer paso es decidir con cuantas variables contamos para el modelo. Si tuvieramos muchas variables explicativas podriamos utilizar el resultado del condes para saber cuáles de ellas utilizar, aunque también sería posible seleccionarlas a partir del análisis de componentes principales. Dado que tenemos poca cantidad de variables usamos todas.   

Empezamos utilizando lm para crear un modelo inicial del cual podemos ir descartando aquellas variables explicativas que nos parecen irrelevantes. Después contrastaremos nuestra selección usando el método Akaike o BIC, que en una sucesión de pasos va descartando variables. #preguntar lo contrastamos con el akaike o nuestro resultado del modelo usamos akaike.

```{r}
# chunk 10
m1<-lm(duration~.,data=df[,c("duration",vars_num)])
summary(m1)
Anova(m1)
```

Viendo este volcado, vemos que todas las variables menos, campaign tienen un p-value superior al 0.05, sin embargo, pdays y previous estan por debajo de 0.1 lo que podriamos llegar a incorporarlas al modelo. El r-square es de 0.006393 lo que nos dice que nuestro modelo no se ajusta bien.

Al ver el resultado de Anova, podemos ver resultados muy parecidos.

Ahora probaremos seleccionando las variables a partir de la criba anterior: #preguntar diferencia summary y Anova, sabemos que el Anova nos dice para cada variable la probabilidad de que el modelo sea igual con o sin esta variable mediante el p-value, entonces que nos dice el summary de difernte?

```{r}
# chunk 20
m2<-lm(duration~campaign+pdays+previous,data=df)
summary(m2)
Anova(m2)

m3<-lm(duration~campaign+pdays,data=df)
summary(m3)
Anova(m3)
vif(m3)

par(mfrow=c(2,2))
plot(m3)
par(mfrow=c(1,1))
m=m3;
```

Viendo el resultado del lm con estas variables, podemos ver que previous da por encima de 0.2, por lo que también descartamos esta variable. También podemos ver que el r-square sigue siendo muy bajo.

Al realizar nuevamente el lm con estas dos variables restantes, vemos que su p-value es inferior al 0.1, por lo que daríamos por concluida la criba.

Finalmente hacemos el análisis de residuos con vif, el cual  nos dice si existen problemas de colinealidad es decir si existen variables que pueden explicar a otras. Si nos da valores por debajo de 3 son buenos y por encima de 5 que las variables elegidas tienen redundancia y que inflará las varianzas. En nuestro caso, el resultado de las dos variables es inferior a 3.

Viendo el plot de la normal Q-Q, vemos que los valores distan mucho de la recta de referencia, con que podemos decir que su distribución no es para nada normal.


Ahora procederemos a comprobar el resultado usando la función step, conocida como Akaike o BIC.

```{r}
# chunk 30
m4<-step(m)
```
Al aplicar el metodo step, vemos que no hace niguna criba más y se queda con el mismo modelo que ya nosotros habiamos cribado. 


Si probamos con la versión bayesiana (del BIC):
```{r}
# chunk 40
m5<-step(m,k=log(nrow(df)))
summary(m5)
par(mfrow=c(2,2))
plot(m5)
par(mfrow=c(1,1))
```

La versión bayesiana es conveniente usarla en casos de muestras grandes. En este caso vemos que se queda con una sola variable (campaign), ya que en el primer step del volcado vemos que sin la variable p-days el valor AIC, en este caso BIC, es menor.

En este caso no podemos hacer el análisis de residuos con vif porque solo tenemos 1 variable.

Al igual que en nuestro caso nos da una plot Q-Q totalmente desviada de las dist normal.


Mediante la función boxcox descartamos la posibilidad de elevar el target al cuadrado, pero sí contemplamos aplicarle el logaritmo, pues el pico de la curva está entre 0 y 1, bastante cerca del 0.
```{r}
# chunk 50
boxcox(m,data=df)
```

Ahora procedemos a la transformación polinómica.  

Como solo tenemos una variable explicativa podemos empezar desde cero pero si tuviéramos ya un modelo no volveríamos a empezar.
```{r}
# chunk 60
m6<-lm(log(duration)~.,data=df[,c("duration",vars_num)]) 
Anova(m6)
```

Viendo el resultado del Anova, procedemos a descartar las variables cuyo valor de Pr es mayor a 0.1

```{r}
# chunk 70
m7<-lm(log(duration)~campaign+pdays+nr.employed,data=df)
summary(m7)
Anova(m7)

par(mfrow=c(2,2))
plot(m7)
par(mfrow=c(1,1))
```
Viendo los p-values, nos encontramos que la variable nr.employed es mayor a 0.1, por lo que procedemos a eliminarla de nuestro modelo.

Relativo al gráfico, podemos ver como la Normal Q-Q ha mejorado bastante acercandose a la recta ideal.

Ahora procedemos a quitar nr.employed.
```{r}
# chunk 80
m9<-lm(log(duration)~campaign+pdays,data=df)
summary(m9)
Anova(m9)
vif(m9)

par(mfrow=c(2,2))
plot(m9)
par(mfrow=c(1,1))
```
Viendo el valor final del r-square, podemos ver que este no es un buen modelo. También los que no puede decir es que las variables no representan a nuestro target, esto ya lo pudimos ver en el deliverable2.

El resultado del vif nos da valores aceptables, diciendo que no hay colinealidad entre variables.

Ahora podemos probar con las versiones cuadráticas de las variables explicativas, partiendo de nuestro mejor modelo:
```{r}
# chunk 90
m20<-lm(log(duration)~poly(campaign,2)+poly(pdays,2),data=df)
summary(m20)
Anova(m20)
vif(m20)
par(mfrow=c(2,2))
plot(m20)
par(mfrow=c(1,1))
influencePlot(m20)
```

Analizando los gráficos:   
  - Residual VS Fitted. En este gráfico muestra los residuos de los valores predecidos. Lo deseable es que los puntos estén uniformemente dispersos, para poderlo contrastar el gráfico esta provisto de una recta smoother que conviene que sea horizontal, y uniforme. En nuestro caso el resultado es aceptable.  
  - Normal Q-Q.  Este plot nos muestra la tendencia a una distribución normal de los residuos, esta provista de una recta diagonal de referencia en la que se espera que los residuos se ajusten lo máximo posible. En nuestro caso, apreciamos ciertas desviaciones en los extremos de la recta, aunque si lo comparamos con plots anteriores, es una recta bastante aceptable.  
  - Scale-Location. Este plot hace referencia a la varianza de los valores de la predicción, si se mantiene constante implica homocedasticidad, de lo contrario heterocedasticidad que se vería reflejada en una nube de puntos en forma de cono. Para nuestro caso, podemos ver que el gráfico tiene una tendencia a cono que además se evidencia con la desviación de la smoother line.  
  - Residuals Vs Leverage.   # preguntar, no vemos las curvas de nivel en el grafico, y como se ven reflejadas las observaciones influyentes, y que significa que el leverage sea el factor de anclaje.
  - InfluenPLot. Nos muestra las individuos más influentes, esto se puede ver gráficamente a través del radio de las circunferencias. En nuestro caso, viendo el gráfico podemos ver que no hay niguna que sea excesivamente influente.


#preguntar practica en  influent data, para estar seguros que el tema del diagnostico, hacer diagnotistico, residual plot, marginal plot, influence plot, porque en algun moemtno hay que hacerlo, se ha explicado esta semana.

Trabajamos con el mejor modelo obtenido, y vemos que individuos influyen más en nuestros datos para saber si estan afectando nuestro resultado.
```{r}
# chunk 100
matplot(dfbetas(m9), type="l", col=2:4,lwd=2)
Boxplot(cooks.distance(m9)) #preguntar nos sale el 3660 pero en el grafico anterior no sale, porque? y si hay que quitarlo al final, porque al final?
```
Consideramos que hay un individuo que repercute demasiado en los datos, aún así no lo eliminaremos hasta el final.


```{r}
# chunk 110
vars_cat_total = c(vars_cat, names(df[,22:29])) # creamos una vars_cat con las  categorias factorizadas
condes(df[,c("duration",vars_cat_total)],1, proba= 0.01)
```
Al hacer condes, con todas las variables categóricas, contemplamos el uso de f.campaign, month, para nuestro modelo ya que la probabilidad de que no tengan relación con el target esta por debajo del 0.01. Como nos sale la versión categórica de campaign que también nos sale en el modelo númerico, debemos elegir entre una u otra pero nunca las dos a la vez.


En vista de que la variable númerica pdays aporta una informaciï¿½n errante ya que aquellos que no fueron contactados tienen asignados un valor que no les corresponde, decidimos utilizar f.pdays porque contiene una informaciï¿½n más rigurosa, ya que se clasifican entre contactados y no contactados.

Contrastamos un modelo con campaign o con f.campaign

```{r}
# chunk 130
m22<-lm(log(duration)~campaign+f.pdays+month,data=df)
m23<-lm(log(duration)~f.pdays+f.campaign+month,data=df)
BIC(m23,m22)

# Ya que nos quedamos con el modelo m22
Anova(m22)
```

Haciendo BIC para comparar modelos, podemos ver que el que da un menor BIC es m22, por lo que decidimos quedarnos con este modelo.


```{r}
# chunk 140
m30<-lm(log(duration)~(campaign+f.pdays+month)^2,data=df)  # preguntar que mejora se tiene con este modelo?
summary(m30)
coef(m30) # preguntar que hace el coef
m31<-step(m30,k=log(nrow(df)))
Anova(m31)
anova(m31,m30) # Fisher test - Priority to BIC criteria

```


```{r}
# chunk 150
# Sin el individuo que más afecta
# m9m<-lm(log(duration)~campaign+pdays,data=df[-3660,]) # ACORDARNOS DE MOVERLO AL FINAL - se deberia eliminar al final de todo no aqui, ya que al final tendremos todos los datos
Boxplot(cooks.distance(m9m))
coef(m9m)
```

# Interactions between numeric variables and factors

```{r}
# Exemple adhoc: Y ~X+A
m40<-lm(log(duration)~campaign+contact,data=df)
summary(m40)

# Suport visual
scatterplot(log(duration)~campaign|contact,data=df)

# Interpretation of models through effects library
library(effects)
plot(allEffects(m40))

m41<-lm(log(duration)~campaign*contact,data=df)
summary(m41)

# Are interactions significant?
anova(m40,m41)  # pvalue << 0.05 -> H0 Rejected -> m41 X*A

plot(allEffects(m41))
```



# Outcome/Target : A binary response variable (Binary Target) will be the response variable for Binary Regression Models included in Statistical Modeling Part III.

Explicative Variables for modeling purposes are those available in dataset, exceptions will be indicated, if any.

Multivariant Analysis conducted in previous deliverables has to be used to select the initial model. Students have some degrees in freedom in model building, but the following conditions are requested:

    * Split the sample in work and test samples (consisting on a 80-20 split). Working data frame has to be used for model building purposes.

    *At least two numerical variables have to be considered as explicative variables for initial steps in model building.
    *Select the most significant factors according to feature selection as initial model factors.  Put some reasonable limits to initial model complexity.
    *You have to consider at least one interaction between a couple of factors and one interaction between factor and covariate.
    *Diagnostics of the final model have to be undertaken. Lack of fit observations and influence data have to be selected and discussed (connections to multidimensional outliers in Multivariant Data Analysis is highly valuable).
    *You have to predict Y (Binary Target)  in the Working Data Frame  vs the rest according to the best validated model that you can find and make a confusion matrix.
    *Make a confusion matrix in the Testing Data Frame for  Y  (Binary Target) according to the best validated model found.

```{r}
#en clase
#adding factors
gm10<-glm(y~pdays+emp.var.rate+poly(previous,2)+cons.price.idx+campaign+cons.conf.idx,family=binomial,data=df)
  
```



Confusion Matrix: When referring to the performance of a classification model, we are interested in the modelâs ability to correctly predict or separate the classes. When looking at the errors made by a classification model, the confusion matrix gives the full picture. Consider e.g. a three class problem with the classes A, and B. The confusion matrix shows how the predictions are made by the model. The rows correspond to the known class of the data, i.e. the labels in the data. The columns correspond to the predictions made by the model. The value of each of element in the matrix is the number of predictions made with the class corresponding to the column for examples with the correct value as represented by the row. Thus, the diagonal elements show the number of correct classifications made for each class, and the off-diagonal elements show the errors made.