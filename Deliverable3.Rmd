---
title: "Deliverable 3"
author: "Guillem Valls, Sergio Mazzariol"
output:
  word_document:
    toc: yes
    toc_depth: '3'
  pdf_document:
    number_sections: yes
    toc: yes
    toc_depth: '3'
  html_document:
    number_sections: yes
    toc: yes
    toc_depth: 3
editor_options:
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r, include=FALSE}
# Load Required Packages: to be increased over the course

requiredPackages <- c("mvoutlier","chemometrics","mice","missForest","missMDA","DMwR","pbkrtest","jomo","readxl","haven","sf","rgdal","missMDA","effects","FactoMineR","car","factoextra","RColorBrewer","ggplot2","dplyr","data.table", "ggmap","ggthemes","knitr","MVA","MASS")
missingPackages <- requiredPackages[!(requiredPackages %in% as.vector(installed.packages(lib.loc="~/R/win-library/3.5")[,"Package"]))]

if(length(missingPackages)) install.packages(missingPackages)
lapply(requiredPackages, require, character.only = TRUE)

```


```{r include=FALSE}
if(Sys.info()[[4]]=="PORT_ELISABET"){
  setwd("C:/Users/usuario/Documents/ADEI/FIB-ADEI-Big-Data-Analysis")
}else{
  setwd("C:/Users/Sergio/Dropbox/UPC/FIB/Analisis de datos y explotacion de la informacion (ADEI)/FIB-ADEI-Big-Data-Analysis")
}

rm(list=ls())
load("data-INI2.RData")
```


El primer paso es decidir con cuantas variables contamos para el modelo. Si tuvieramos muchas variables explicativas podriamos utilizar el resultado del condes para saber cu�les de ellas utilizar, aunque tambi�n ser�a posible seleccionarlas a partir del an�lisis de componentes principales. Dado que tenemos poca cantidad de variables usamos todas.   

Empezamos utilizando lm para crear un modelo inicial del cual podemos ir descartando aquellas variables explicativas que nos parecen irrelevantes. Despu�s contrastaremos nuestra selecci�n usando el m�todo Akaike o BIC, que en una sucesi�n de pasos va descartando variables.

```{r}
# chunk 10
m1<-lm(duration~.,data=df[,c("duration",vars_num)])
summary(m1)
Anova(m1)
```

Viendo este volcado, vemos que todas las variables menos, campaign tienen un p-value superior al 0.05, sin embargo, pdays y previous estan por debajo de 0.1 lo que podriamos llegar a incorporarlas al modelo. El r-square es de 0.006393 lo que nos dice que nuestro modelo no se ajusta bien.

Al ver el resultado de Anova, podemos ver resultados muy parecidos.

Ahora probaremos seleccionando las variables a partir de la criba anterior:

```{r}
# chunk 20
m2<-lm(duration~campaign+pdays+previous,data=df)
summary(m2)
Anova(m2)

m3<-lm(duration~campaign+pdays,data=df)
summary(m3)
Anova(m3)
vif(m3)

par(mfrow=c(2,2))
plot(m3)
par(mfrow=c(1,1))
m=m3;
```

Viendo el resultado del lm con estas variables, podemos ver que previous da por encima de 0.2, por lo que tambi�n descartamos esta variable. Tambien podemos ver que el r-square sigue siendo muy bajo.

Al realizar nuevamente el lm con estas dos variables restantes, vemos que su p-value es inferior al 0.1, por lo que dar�amos por concluida la criba.

Finalmente hacemos el an�lisis de residuos con vif, el cual nos dice que si nos da valores por debajo de 3 son buenos y por encima de 5 que las variables elegidas tienen redundancia y que inflar� las varianzas. En nuestro caso, el resultado de las dos variables es inferior a 3.

Viendo el plot de la normal Q-Q, vemos que los valores distan mucho de la recta de referencia, con que podemos decir que su distribuci�n no es para nada normal.


Ahora procederemos a comprobar el resultado usando la funci�n step, conocida como Akaike o BIC.

```{r}
# chunk 30
m4<-step(m)
```
Al aplicar el metodo step, vemos que no hace niguna criba m�s y se queda con el mismo modelo que ya nosotros habiamos cribado. 


Si probamos con la versi�n bayesiana (del BIC):
```{r}
# chunk 40
m5<-step(m,k=log(nrow(df)))
summary(m5)
par(mfrow=c(2,2))
plot(m5)
par(mfrow=c(1,1))
```

La versi�n bayesiana es conveniente usarla en casos de muestras grandes. En este caso vemos que se queda con una sola variable (campaign), ya que el p-value de pdays es mayor a 0.05.

En este caso no podemos hacer el an�lisis de residuos con vif porque solo tenemos 1 variable.

Al igual que en nuestro caso nos da una plot Q-Q totalmente desviada de las dist normal.


Mediante la funci�n boxcox descartamos la posibilidad de elevar el target al cuadrado, pero s� contemplamos aplicarle el logaritmo, pues el pico de la curva est� entre 0 y 1 y bastante cerca del 0.
```{r}
# chunk 50
boxcox(m,data=df)
```


Ahora procedemos a la transformaci�n polin�mica.  

Como solo tenemos una variable explicativa podemos empezar desde cero pero si tuvi�ramos ya un modelo no volver�amos a empezar.
```{r}
# chunk 60
m6<-lm(log(duration)~.,data=df[,c("duration",vars_num)]) 
Anova(m6)
```

Viendo el resultado del Anova, procedemos a descartar las variables cuyo valor de Pr es mayor a 0.1

```{r}
# chunk 70
m7<-lm(log(duration)~campaign+pdays+nr.employed+euribor3m,data=df)
summary(m7)
Anova(m7)
vif(m7)

par(mfrow=c(2,2))
plot(m7)
par(mfrow=c(1,1))
```
Al hacer vif, podemos ver que nr.employed y euribor3m, tienen valores mayores de 5, lo que nos dice que hay redundancia entre estas variables por lo que procedemos a quitar la que tiene mayot Pr (euribor3m).

Relativo al gr�fico, podemos ver como la Normal Q-Q ha mejorado bastante acercandose a la recta ideal.

```{r}
# chunk 80
m8<-lm(log(duration)~campaign+pdays+nr.employed,data=df)
summary(m8)
Anova(m8)
vif(m8)
par(mfrow=c(2,2))
plot(m8)
par(mfrow=c(1,1))
```
Ahora al hacer vif con las nuevas variables vemos que los valores dan menores de 3, pero el Pr de employed es mayor a 0.1, por que decidimos sacarla.

No se ve mucha diferencia respecto a el grafico de m7.

Ahora procedemos a quitar nr.employed.
```{r}
# chunk 90
m9<-lm(log(duration)~campaign+pdays,data=df)
summary(m9)
Anova(m9)
vif(m9)

par(mfrow=c(2,2))
plot(m9)
par(mfrow=c(1,1))
```
Viendo el valor final del r-square, podemos ver que este no es un buen modelo. Tambien los que no puede decir es que las variables no representan a nuestro target, estoy ya lo pudimos ver en el deliverable2.


Ahora podemos probar con las versiones cuadr�ticas de las variables explicativas:
```{r}
# chunk 100
m20<-lm(log(duration)~poly(campaign,2)+poly(pdays,2),data=df)
summary(m20)
Anova(m20)
vif(m20)
par(mfrow=c(2,2))
plot(m20)
par(mfrow=c(1,1))
```

# practica en  influent data, para estar seguros que el tema del diagnostico, hacer diagnotistico, residual plot, marginal plot, influence plot, porque en algun moemtno hay que hacerlo, se ha explicado esta semana.

Trabajamos con el mejor modelo obtenido, y vemos que individuos influyen m�s en nuestros datos para saber si estan afectando nuestro resultado.
```{r}
# chunk 110
matplot(dfbetas(m9), type="l", col=2:4,lwd=2)
Boxplot(cooks.distance(m9))
coef(m9)
# Sin el individuo que m�s afecta
# m9m<-lm(log(duration)~campaign+pdays,data=df[-3660,]) # ACORDARNOS DE MOVERLO AL FINAL - se deberia eliminar al final de todo no aqui, ya que al final tendremos todos los datos
Boxplot(cooks.distance(m9m))
coef(m9m)
```
Consideramos que hay un individuo que repercute demasiado en los datos, a�n as� no lo eliminaremos hasta el final.


```{r}
# chunk 120
vars_cat_total = c(vars_cat, names(df[,22:29])) # creamos una vars_cat con las  categorias factorizadas
condes(df[,c("duration",vars_cat_total)],1, proba= 0.01)
```
Al hacer condes, con todas las variables categoricas, contemplamos el uso de f.campaign, month, para nuestro modelo ya que la probabilidad de que no tengan relaci�n con el target esta por debajo del 0.01. Como nos sale la versi�n categorica de campaign que tambi�n nos sale en el modelo n�merico, debemos elegir entre una u otra pero nunca las dos a la vez.


En vista de que la variable n�merica pdays aporta una informaci�n errante ya que aquellos que no fueron contactados tienen asignados un valor que no les corresponde, decidimos utilizar f.pdays porque contiene una informaci�n m�s rigurosa, ya que se clasifican entre contactados y no contactados.

Contrastamos un modelo con campaign o con f.campaign

```{r}
# chunk 130
m22<-lm(log(duration)~campaign+f.pdays+month,data=df)
m23<-lm(log(duration)~f.pdays+f.campaign+month,data=df)
BIC(m23,m22)

# Ya que nos quedamos con el modelo m22
Anova(m22)
```

Haciendo BIC para comparar modelos, podemos ver que el que da un menor BIC es m22, por lo que decidimos quedarnos con este modelo.


```{r}
# chunk 140
m30<-lm(log(duration)~(campaign+f.pdays+month)^2,data=df)  # preguntar que mejora se tiene con este modelo?
summary(m30)
coef(m30) # preguntar que hace el coef
m31<-step(m30,k=log(nrow(df)))
Anova(m31)
anova(m31,m30) # Fisher test - Priority to BIC criteria

```


```{r}
# chunk 150

```

# Interactions between numeric variables and factors

```{r}
# Exemple adhoc: Y ~X+A
m40<-lm(log(duration)~campaign+contact,data=df)
summary(m40)

# Suport visual
scatterplot(log(duration)~campaign|contact,data=df)

# Interpretation of models through effects library
library(effects)
plot(allEffects(m40))

m41<-lm(log(duration)~campaign*contact,data=df)
summary(m41)

# Are interactions significant?
anova(m40,m41)  # pvalue << 0.05 -> H0 Rejected -> m41 X*A

plot(allEffects(m41))
```



# Outcome/Target : A binary response variable (Binary Target) will be the response variable for Binary Regression Models included in Statistical Modeling Part III.

Explicative Variables for modeling purposes are those available in dataset, exceptions will be indicated, if any.

Multivariant Analysis conducted in previous deliverables has to be used to select the initial model. Students have some degrees in freedom in model building, but the following conditions are requested:

    * Split the sample in work and test samples (consisting on a 80-20 split). Working data frame has to be used for model building purposes.

    *At least two numerical variables have to be considered as explicative variables for initial steps in model building.
    *Select the most significant factors according to feature selection as initial model factors.  Put some reasonable limits to initial model complexity.
    *You have to consider at least one interaction between a couple of factors and one interaction between factor and covariate.
    *Diagnostics of the final model have to be undertaken. Lack of fit observations and influence data have to be selected and discussed (connections to multidimensional outliers in Multivariant Data Analysis is highly valuable).
    *You have to predict Y (Binary Target)  in the Working Data Frame  vs the rest according to the best validated model that you can find and make a confusion matrix.
    *Make a confusion matrix in the Testing Data Frame for  Y  (Binary Target) according to the best validated model found.

```{r}
#en clase
#adding factors
gm10<-glm(y~pdays+emp.var.rate+poly(previous,2)+cons.price.idx+campaign+cons.conf.idx,family=binomial,data=df)
  
```



Confusion Matrix: When referring to the performance of a classification model, we are interested in the model’s ability to correctly predict or separate the classes. When looking at the errors made by a classification model, the confusion matrix gives the full picture. Consider e.g. a three class problem with the classes A, and B. The confusion matrix shows how the predictions are made by the model. The rows correspond to the known class of the data, i.e. the labels in the data. The columns correspond to the predictions made by the model. The value of each of element in the matrix is the number of predictions made with the class corresponding to the column for examples with the correct value as represented by the row. Thus, the diagonal elements show the number of correct classifications made for each class, and the off-diagonal elements show the errors made.