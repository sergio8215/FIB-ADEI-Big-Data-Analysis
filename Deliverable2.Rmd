---
title: "Deliverable 2"
author: "Guillem Valls, Sergio Mazzariol"
output:
  word_document:
    toc: yes
    toc_depth: '3'
  pdf_document:
    number_sections: yes
    toc: yes
    toc_depth: '3'
  html_document:
    number_sections: yes
    toc: yes
    toc_depth: 3
editor_options:
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r, include=FALSE}
# Load Required Packages: to be increased over the course

requiredPackages <- c("mvoutlier","chemometrics","mice","missForest","missMDA","DMwR","pbkrtest","jomo","readxl","haven","sf","rgdal","missMDA","effects","FactoMineR","car","factoextra","RColorBrewer","ggplot2","dplyr","data.table", "ggmap","ggthemes","knitr","MVA")
missingPackages <- requiredPackages[!(requiredPackages %in% as.vector(installed.packages(lib.loc="~/R/win-library/3.5")[,"Package"]))]

if(length(missingPackages)) install.packages(missingPackages)
lapply(requiredPackages, require, character.only = TRUE)

```


```{r include=FALSE}
#setwd("C:/Users/Sergio/Dropbox/UPC/FIB/Analisis de datos y explotacion de la informacion (ADEI)/FIB-ADEI-Big-Data-Analysis")
setwd("C:/Users/usuario/Documents/ADEI/FIB-ADEI-Big-Data-Analysis")

rm(list=ls())
load("data-INI2.RData")
```


##Valores propios y ejes dominantes

### Eigenvalues and dominant axes analysis. How many axes we have to interpret according to Kaiser and Elbow's rule?

Hemos decidido probar como se ve el PCA sin y con la variable pdays, ya que consideramos con una variable con bastantes missings, aún así aporta información por lo tanto la vamos a considerar.

```{r}
vars_num_sin_pday = vars_num[-3];
par(mfrow=c(1,2))
res2.pca<-PCA(df[,c('duration',vars_num_sin_pday)],quanti.sup=1)
res.pca<-PCA(df[,c('duration',vars_num)],quanti.sup=1)
par(mfrow=c(1,1))
```


Vemos que con pdays existe una relación inversa con previous, respecto a los dos ejes factoriales, sin pdays se puede ver que la contribución de la variable age con el segundo eje factorial es mayor, ya que graficamente tiene mayor magnitud ademas que las variables socio economicas, se ven mejor representadas en el primer eje factorial.


Por la ley de Kaiser, deberiamos utilizar los 3 primeros ejes factoriales, los cuales son mayores a 1. 
Por la ley de ElBow, al realizar el grafico podemos ver que la grafica empieza a ser plana a partir de la 2da dimension, es decir que se cogen las 2 primeras dimensiones.
 Si tomamos en cuenta el criterio del 80% se deberian coger las 4 primeras dimensiones.
 
 Para realizar el futuro analisi, combiene utilizar dimensiones pares, por lo que decidimos de solo usar 2.

```{r}
summary(res.pca,ncp=4,nb.dec=2)
plot(res.pca$eig[,1],main="Eigenvalues",type="o", col="blue")
```


#  Individuals point of view: Are they any individuals "too contributive"? To better understand the axes meaning use the extreme individuals. Detection of multivariant outliers and influent data.

Primero graficamos en rp los 15 individuos más contributivos en ambos ejes, luego analizamos los 5 individuos más contributivos en la dimensión 1 y 2. Al ver si estos tienen alguna relación significante, podemos decir que para los 5 individuos de la dimension 1, vemos que principalmente son gente mayor de 45años, todos han comprado el producto, han sido contactados mediante el móvil, han sido contactados previamente, comprado un producto en una campaña anterior, la duración de la llamada ha sido mayor a los 300s.

Para la dimension 2 podemos ver practicamente las mismas caracteristas menos la duración que ha sido menor. Cabe a destacar que sin embargo hay dos individuos que son muy contributivos en ambos ejes, eso hace pensar que pueden ser posibles outliers pero de igual forma los dejamos en los datos. 


```{r}

plot.PCA(res.pca, choix=c("ind"),cex=0.8,col.ind="grey70",select="contrib15",axes=c(1,2)) 
mas_ctr_dim1 <- sort(res.pca$ind$contrib[,1], decreasing = TRUE)[1:5]
mas_ctr_dim2 <- sort(res.pca$ind$contrib[,2], decreasing = TRUE)[1:5] # ver si hay alguna caracteristica relevnte, ver los 5 mas relevantes de este eje y del eje 2, ver que comparten en común.

# mas_ctr_dim1
#    40396     39592     38902     38814     36461 
#0.2482556 0.2196878 0.2057888 0.2053101 0.1994545  
# mas_ctr_dim2
#    40396     40703     39592     38902     39612 
#1.3837961 1.1876086 0.8973750 0.8642930 0.8630042 

df[names(mas_ctr_dim1),]
df[names(mas_ctr_dim2),]

# which(res.pca$ind$dist>=max(res.pca$ind$dist)) # encontrar el ind con mayor distancia al centro

# pendiente apuntar los indv mas ctr en colores diferente

```


### Interpreting the axes:  Variables point of view coordinates, quality of representation, contribution of the variables  

Al hacer el PCA con la variable target duration como suplementaria, podemos ver que su modulo es practicamente nulo, esto quiere decir que la variable no se ve representada en niguno de los ejes factoriles.

El eje horizontal está muy relacionado con las variables socio economicas, mirando el cos2 del summary podemos ver que las variables que estan mejor representadas con la dimension 1 son:
  euribor3m, emp.var.rate, nr.emplyed
Para el eje vertical: 
  pdays y previous

Para el eje vertical, podemos decir que esta relacionado con las campañas previas.

Al hacer el PCA con la variable target Y como suplementaria, podemos ver que en el grafico de rp, el factor no, esta muy cerca del centro de gravedad, por lo que no se ve representada en estos ejes factoriales. En cambio el factor si, esta con una distancia mayor del centro de gravedad, aunque poco significativa.

```{r}
par(mfrow=c(1,2))
res.pca<-PCA(df[,c('duration',vars_num)],quanti.sup=1)
summary(res.pca,ncp=4,nb.dec=2)
res.pca<-PCA(df[,c('y',vars_num)],quali.sup=1)
summary(res.pca,ncp=4,nb.dec=2)
par(mfrow=c(1,1))

```


### Perform a PCA taking into account also supplementary variables the supplementary variables can be quantitative and/or categorical

Hemos dividido el plot en diferentes partes, para así poder entender y ver mejor el resultado.
Para el primer plot, tenemos f.job y f.season para los cuales podemos ver que 


```{r}

# preguntar y pendiente cambiar de color cada variable con sus factores

 vars_factorizadas<- c("f.job","f.season","f.education","f.age","f.duration","f.campaign","f.pdays","f.previous","y");
# vars_factorizadas<- c("f.job","f.season","f.education","f.age","f.pdays","y");

res.pca<-PCA(df[,c('duration',vars_num, "f.job","f.season")],quanti.sup=1, quali.sup = c(11:12))
plot.PCA(res.pca,choix="ind",invisible="ind",cex=0.75)

res.pca<-PCA(df[,c('duration',vars_num,"f.education","f.age")],quanti.sup=1, quali.sup = c(11:12))
plot.PCA(res.pca,choix="ind",invisible="ind",cex=0.75)

res.pca<-PCA(df[,c('duration',vars_num,"f.pdays","y")],quanti.sup=1, quali.sup = c(11:12))
plot.PCA(res.pca,choix="ind",invisible="ind",cex=0.75)

res.pca<-PCA(df[,c('duration',vars_num,vars_factorizadas)],quanti.sup=1, quali.sup = c(11:19))

plot.PCA(res.pca,choix="ind",invisible="ind")# no ponerlas todas a la vez, poner por grupos. Pntar grupos de variables, si categorias quedan en el medio no valen de nada, hay que ver si categorias de las variables que estan lejos del centrode gravedad, son las que valen la pena. Fijarse que no cambian los valores de los ejes porque si no no tendremos referencia para comparar

summary(res.pca,dig = 2, nbelements= 50, nbind=3, ncp=4)


#lines(res.pca$quali.sup$coord[1:2,1:2],col="blue",lwd=2)
#lines(res.pca$quali.sup$coord[3:6,1:2],col="cyan",lwd=2)

```

## K-Means Classification

Hemos graficado los grupos separados en 3, 4, 5 y 6 clusters, para los cuales nos parece que graficamente con 4 clusters los grupos están bien definidos, por lo que decidimos usar con 4 clusters los cuals analisaremos seguidamente.


```{r}
dclu<- res.pca$ind$coord[,1:2]; # los dos ejes

kcla<- kmeans(dclu,4);

df$cluster3 = factor(kmeans(dclu,3)$cluster);
df$cluster4 = factor(kmeans(dclu,4)$cluster);
df$cluster5 = factor(kmeans(dclu,5)$cluster);
df$cluster6 = factor(kmeans(dclu,6)$cluster);

#summary(kcla)
#table(kcla$cluster)
#kcla$betweenss/kcla$totss

par(mfrow=c(2,2))
res.pca<-PCA(df[,c('duration',vars_num, "cluster3")],quanti.sup=1, quali.sup = 11, graph=FALSE)
plot.PCA(res.pca,choix="ind",habillage=11,select=0 ,cex=0.75)
res.pca<-PCA(df[,c('duration',vars_num, "cluster4")],quanti.sup=1, quali.sup = 11, graph=FALSE)
plot.PCA(res.pca,choix="ind",habillage=11,select=0 ,cex=0.75)
res.pca<-PCA(df[,c('duration',vars_num, "cluster5")],quanti.sup=1, quali.sup = 11, graph=FALSE)
plot.PCA(res.pca,choix="ind",habillage=11,select=0 ,cex=0.75)
res.pca<-PCA(df[,c('duration',vars_num, "cluster6")],quanti.sup=1, quali.sup = 11, graph=FALSE)
plot.PCA(res.pca,choix="ind",habillage=11,select=0 ,cex=0.75)
par(mfrow=c(1,1))


df <- df[,c(1:29, 31)] # guardamos la clasificación en 4 clusters

```
 

## Description of clusters

Viendo el chi-square test, podemos saber que variables se utilizarán para caracterizar nuestros 4 clusters. 
Viendo las categorias donde el P-value es casi 0 podemos ver que las categorias que cumplen estas caracteristicas son: month (y por extensión también f.season), poutcome, f.pdays, f.previous, contact, y, job(f.job), default, f.age, f.campaign, marital. Ahora veremos que cateogrias de estas variables son las que caracterizan estos clusters.

Para el catdes del cluster 1, hemos podido ver las categorias que mejor lo definen, la temporada de verano es la que mejor lo caracteriza, f.season.Jun-Aug, tenemos también un poutcome que nos indica que ninguno de los individuos ha sido contactado previamente, podemos ver que y.no representa el 95% de este cluster, han sido contactados en su mayoria por teléfono local, también podemos ver que este cluster esta ligeramente relacionado con la categoria f.job.Serv-Tech-BlueC. En conclusión podemos decir que este cluster esta caracterizado por:
	- Meses de jun-Ago
	- No han sido contactados previamente
	- Han sido contactados más de una vez en la campaña actual
	- Contactados por teléfono fijo
	- Trabajo es en media servicio, tecnicos o blue collar.
	- No compraron el producto en su mayoria.
	
Para el cluster 2, tenemos:
	- La temporada de mar-may están sobrerepresentadas en este cluster
	- Han sido contacados en su mayoria por teléfono móvil
	- No han comprado el producto en campañas anteriores
	- Han sio contacados en camapañas previas
	- La categoria student esta sobrerepresentada
	- La aceptación del producto esta sobrerepresentada también
	
Para el cluster 3, tenemos:
	- Temporada de Sep-Dec
	- Contactdos por móvil en su mayoria
	- La categoria de job.management esta sobrerepresentada
	- No han adquirido el producto
	- Una gran cantidad de individuos rechazo el productoy y.no
	
Para el cluster 4, podemos ver que aglutina individuos muy bien caracterizado por las siguientes variables:
	- Han sido contactados previamente f.pdays[0,22]
	- Han comprado el producto en una campaña previa
	- Han comprado el producto y.yes
	- Temporada de Sep-Dec
	- Han sido contactados por móvil
	- Una parte importante son job.retired
	- Una edad de f.age-(50,92]
	
# preguntar si hay que analizar Description of each cluster by quantitative variables de catdes.

```{r}
catdes(df, 30, proba = 0.001)
```


## Hierarchical Clustering

Al hacer el HCPC podemos ver que el grafico de ganancia de inercia, nos da para dos variables en su mayoria, y luego dos picos más pequeños.

Analizando el resultado del HCPC podemos ver lo siguiente para cada cluster.


Ahora vemos el clustering no supervisado. Vamos a clasificar estos clusters.
Para el cluster 1:
- Esta caracterizado por personas que han sido contactados previamente
- Han aceptado el producto
- Se han contactado en f.season.Sep-Dec
- Tienen una sobrerepresentación de f.job.Entrep-Retired-selfEmpl
- Llamadas de duración mayor a 3min

Para el cluster 2:
- Han sido contactados f.season.Mar-May
- Han sido contactados en campañas previas
- Han aceptado el producto y.yes


Para el cluster 3:
- Han sido contactados previamente
- No han sido contactados en campañas previas
- Han sido contactados en la temporada de f.season.Jun-Aug
- Han rechazado el producto
- Tienen una leve represetnación de f.job.Serv-Tech-BlueC

```{r}

res.pca<-PCA(df[,c('duration',vars_num,vars_factorizadas)],quanti.sup=1, quali.sup = c(11:19), ncp=2, graph=FALSE)

#res.pca<-PCA(df[,c('duration',vars_num, "cluster4")],quanti.sup=1, quali.sup = 11, graph=FALSE)
#plot.PCA(res.pca,choix="ind",habillage=11,select=0 ,cex=0.75)

res.hcpc<-HCPC(res.pca,order=TRUE)
attributes(res.hcpc)

summary(res.hcpc$data.clust)
# var save  <-res.hcpc$data.clust$clust

attributes(res.hcpc$desc.var)

# Factors globally related to clustering partition
res.hcpc$desc.var$test.chi2

# Categories over/under represented in each cluster
res.hcpc$desc.var$category

# Numeric variables globally related to clustering partition
res.hcpc$desc.var$quanti.var
res.hcpc$desc.var$quanti

# Manera ràpida: proper dia

### desc.ind ###
### C. The description of the clusters by the individuals ###
names(res.hcpc$desc.ind)
res.hcpc$desc.ind$para  # Close to center of gravity
res.hcpc$desc.ind$dist  
```


## Description of clusters
```{r}


```


## CA analysis for your data should contain your factor version of the numeric target (previous) in K= 7 (maximum 10) levels and 2 factors:
### Eigenvalues and dominant axes analysis. How many axes we have to consider are there any row categories that can be combined/avoided to explain Duration target.

Cogemos las dimensiones que son mayores a la media, para nuestro caso solo seria 1(kaiser).


```{r}
# CA - f.duration vs f.age
res.ca<-CA(table(df$f.age,df$f.duration))
attributes(res.ca)
res.ca$eig
mean(res.ca$eig[,1])  # Mean of eigenvalues
sum(res.ca$eig[,1])  # Total inertia

# Rows
res.ca$row
# Columns: the same
res.ca$col

# Link levels in rows
plot.CA(res.ca)
lines(res.ca$row$coord[,1],res.ca$row$coord[,2],lwd=2,col="darkblue")
lines(res.ca$col$coord[,1],res.ca$col$coord[,2],lwd=2,col="red")

# Phi2 = Intensity of the association Chisq/nbobservations
sum(res.ca$eig[,1]) # Total Inertia = Phi2
# H0: f.duration - f.age independency
chisq.test(table(df$f.age,df$f.duration))
7.2021/length(df[,1])

# Repeat job vs f.duration
par(cex=0.8)
res.ca<-CA(table(df$job,df$f.duration))
res.ca$eig
mean(res.ca$eig[,1])

# Target y : no apply
res.ca<-CA(table(df$y,df$f.duration))
res.ca
plot.CA(res.ca)

# Traditional analysis
table(df$y,df$f.duration)
chisq.test(table(df$y,df$f.duration))
```


